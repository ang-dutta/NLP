{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae210548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b036e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def read_corpus(path):\n",
    "    word_counts = Counter()\n",
    "    with io.open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokens = line.split()\n",
    "            for tok in tokens:\n",
    "                word_counts[tok] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a60abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# BPE Implementation\n",
    "# -----------------------\n",
    "def get_word_as_symbols(word):\n",
    "    return list(word) + [\"</w>\"]\n",
    "\n",
    "def build_token_sequences(word_counts):\n",
    "    seqs = {}\n",
    "    for w, c in word_counts.items():\n",
    "        syms = tuple(get_word_as_symbols(w))\n",
    "        seqs[syms] = seqs.get(syms, 0) + c\n",
    "    return seqs\n",
    "\n",
    "def get_pair_frequencies(seqs):\n",
    "    pairs = Counter()\n",
    "    for symbols, count in seqs.items():\n",
    "        if len(symbols) < 2: \n",
    "            continue\n",
    "        prev = symbols[0]\n",
    "        for s in symbols[1:]:\n",
    "            pairs[(prev, s)] += count\n",
    "            prev = s\n",
    "    return pairs\n",
    "\n",
    "def merge_pair_in_seq(seq, pair):\n",
    "    a,b = pair\n",
    "    out = []\n",
    "    i = 0\n",
    "    n = len(seq)\n",
    "    while i < n:\n",
    "        if i < n-1 and seq[i] == a and seq[i+1] == b:\n",
    "            out.append(a + b)\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(seq[i])\n",
    "            i += 1\n",
    "    return tuple(out)\n",
    "\n",
    "def train_bpe(word_counts, max_merges=None, target_vocab_size=None, report_every=1000):\n",
    "    seqs = build_token_sequences(word_counts)\n",
    "    merges = []\n",
    "    vocab = set()\n",
    "    for s in seqs:\n",
    "        for sym in s:\n",
    "            vocab.add(sym)\n",
    "    initial_vocab_size = len(vocab)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        pairs = get_pair_frequencies(seqs)\n",
    "        if not pairs:\n",
    "            break\n",
    "        most_common_pair, freq = pairs.most_common(1)[0]\n",
    "        merges.append(most_common_pair)\n",
    "        new_seqs = {}\n",
    "        for symbols, count in seqs.items():\n",
    "            new_symbols = merge_pair_in_seq(symbols, most_common_pair)\n",
    "            new_seqs[new_symbols] = new_seqs.get(new_symbols, 0) + count\n",
    "        seqs = new_seqs\n",
    "        a,b = most_common_pair\n",
    "        vocab.add(a + b)\n",
    "        if max_merges is not None and len(merges) >= max_merges:\n",
    "            break\n",
    "        if target_vocab_size is not None and len(vocab) >= target_vocab_size:\n",
    "            break\n",
    "        if iteration % report_every == 0:\n",
    "            print(f\"[BPE] Iter {iteration}: merges={len(merges)}, vocab_size={len(vocab)}, top_pair={most_common_pair}, freq={freq}\")\n",
    "    return merges, vocab, seqs\n",
    "\n",
    "def save_merges(merges, path):\n",
    "    with io.open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        for a,b in merges:\n",
    "            f.write(f\"{a} {b}\\n\")\n",
    "\n",
    "def save_vocab(vocab, path):\n",
    "    with io.open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        for tok in sorted(vocab):\n",
    "            f.write(tok + \"\\n\")\n",
    "\n",
    "def bpe_encode_word(word, merges):\n",
    "    symbols = list(word) + [\"</w>\"]\n",
    "    merge_map = { (a,b): a+b for (a,b) in merges }\n",
    "    seq = tuple(symbols)\n",
    "    changed = True\n",
    "    for pair in merges:\n",
    "        seq = merge_pair_in_seq(seq, pair)\n",
    "    tokens = list(seq)\n",
    "    if tokens and tokens[-1] == \"</w>\":\n",
    "        tokens = tokens[:-1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd0a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# WordPiece-style Implementation (frequency-driven)\n",
    "# -----------------------\n",
    "def train_wordpiece_like(word_counts, target_vocab_size=None, max_merges=None, max_subword_len=100, report_every=1000):\n",
    "    seqs = build_token_sequences(word_counts)\n",
    "    vocab = set()\n",
    "    for s in seqs:\n",
    "        for sym in s:\n",
    "            vocab.add(sym)\n",
    "    merges = []\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        substr_freq = Counter()\n",
    "        for symbols, count in seqs.items():\n",
    "            L = len(symbols)\n",
    "            for i in range(L):\n",
    "                for j in range(i+2, min(L, i+max_subword_len)+1):\n",
    "                    sub = tuple(symbols[i:j])\n",
    "                    if any(sym == \"</w>\" for sym in sub[:-1]):\n",
    "                        continue\n",
    "                    substr_freq[sub] += count\n",
    "        if not substr_freq:\n",
    "            break\n",
    "        candidate, freq = substr_freq.most_common(1)[0]\n",
    "        if freq <= 0:\n",
    "            break\n",
    "        new_token = \"\".join(candidate)\n",
    "        merges.append(candidate)\n",
    "        new_seqs = {}\n",
    "        for symbols, count in seqs.items():\n",
    "            s_list = list(symbols)\n",
    "            i = 0\n",
    "            out = []\n",
    "            while i < len(s_list):\n",
    "                if i + len(candidate) <= len(s_list) and tuple(s_list[i:i+len(candidate)]) == candidate:\n",
    "                    out.append(new_token)\n",
    "                    i += len(candidate)\n",
    "                else:\n",
    "                    out.append(s_list[i])\n",
    "                    i += 1\n",
    "            new_seqs[tuple(out)] = new_seqs.get(tuple(out), 0) + count\n",
    "        seqs = new_seqs\n",
    "        vocab.add(new_token)\n",
    "        if max_merges is not None and len(merges) >= max_merges:\n",
    "            break\n",
    "        if target_vocab_size is not None and len(vocab) >= target_vocab_size:\n",
    "            break\n",
    "        if iteration % report_every == 0:\n",
    "            print(f\"[WP] Iter {iteration}: merges={len(merges)}, vocab_size={len(vocab)}, chosen_len={len(candidate)}, freq={freq}\")\n",
    "    return merges, vocab, seqs\n",
    "\n",
    "def wordpiece_encode_word(word, merges):\n",
    "    added_tokens = {\"\".join(m): True for m in merges}\n",
    "    chars = list(word)\n",
    "    i = 0\n",
    "    out = []\n",
    "    while i < len(chars):\n",
    "        matched = None\n",
    "        maxlen = 0\n",
    "        for L in range(len(chars)-i, 0, -1):\n",
    "            candidate = \"\".join(chars[i:i+L])\n",
    "            if candidate in added_tokens or L == 1:\n",
    "                matched = candidate\n",
    "                maxlen = L\n",
    "                break\n",
    "        if matched is None:\n",
    "            matched = chars[i]\n",
    "            maxlen = 1\n",
    "        out.append(matched)\n",
    "        i += maxlen\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus...\n",
      "Unique word types: 38949, token (types) example: নয়াদিল্লি\n",
      "Training BPE with fixed merge steps = 32000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Command-line / Run\n",
    "# -----------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Train BPE and WordPiece-style tokenizers.\")\n",
    "    parser.add_argument(\"--input\", type=str, default=\"bengali_tokenized.txt\", help=\"Path to tokenized text (space-separated tokens).\")\n",
    "    parser.add_argument(\"--bpe_merges\", type=int, default=32000, help=\"Number of BPE merges to run (if set).\")\n",
    "    parser.add_argument(\"--wp_merges\", type=int, default=32000, help=\"Number of WordPiece-style merges to run (if set).\")\n",
    "    parser.add_argument(\"--vocab_size\", type=int, default=32000, help=\"Target vocab size for vocab-driven training (if used).\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Reading corpus...\")\n",
    "    word_counts = read_corpus(args.input)\n",
    "    print(f\"Unique word types: {len(word_counts)}, token (types) example: {next(iter(word_counts))}\")\n",
    "\n",
    "    print(\"Training BPE with fixed merge steps =\", args.bpe_merges)\n",
    "    merges_bpe_fixed, vocab_bpe_fixed, seqs_bpe_fixed = train_bpe(\n",
    "        word_counts, max_merges=args.bpe_merges, target_vocab_size=None, report_every=2000\n",
    "    )\n",
    "    save_merges(merges_bpe_fixed, f\"merges_bpe_{args.bpe_merges}merges.txt\")\n",
    "    save_vocab(vocab_bpe_fixed, f\"vocab_bpe_{args.bpe_merges}merges.txt\")\n",
    "    print(\"Saved BPE merges and vocab for fixed merges.\")\n",
    "\n",
    "    print(\"Training BPE until vocab size reaches\", args.vocab_size)\n",
    "    merges_bpe_vocab, vocab_bpe_vocab, seqs_bpe_vocab = train_bpe(\n",
    "        word_counts, max_merges=None, target_vocab_size=args.vocab_size, report_every=2000\n",
    "    )\n",
    "    save_merges(merges_bpe_vocab, f\"merges_bpe_vocab_{args.vocab_size}.txt\")\n",
    "    save_vocab(vocab_bpe_vocab, f\"vocab_bpe_vocab_{args.vocab_size}.txt\")\n",
    "    print(\"Saved BPE merges and vocab for vocab-size target.\")\n",
    "\n",
    "    print(\"Training WordPiece-style with fixed merges =\", args.wp_merges)\n",
    "    merges_wp_fixed, vocab_wp_fixed, seqs_wp_fixed = train_wordpiece_like(\n",
    "        word_counts, target_vocab_size=None, max_merges=args.wp_merges, report_every=2000\n",
    "    )\n",
    "    with io.open(f\"merges_wp_{args.wp_merges}merges.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "        for m in merges_wp_fixed:\n",
    "            f.write(\" \".join(m) + \"\\n\")\n",
    "    save_vocab(vocab_wp_fixed, f\"vocab_wp_{args.wp_merges}merges.txt\")\n",
    "    print(\"Saved WordPiece-style merges and vocab for fixed merges.\")\n",
    "\n",
    "    print(\"Training WordPiece-style until vocab size reaches\", args.vocab_size)\n",
    "    merges_wp_vocab, vocab_wp_vocab, seqs_wp_vocab = train_wordpiece_like(\n",
    "        word_counts, target_vocab_size=args.vocab_size, max_merges=None, report_every=2000\n",
    "    )\n",
    "    with io.open(f\"merges_wp_vocab_{args.vocab_size}.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "        for m in merges_wp_vocab:\n",
    "            f.write(\" \".join(m) + \"\\n\")\n",
    "    save_vocab(vocab_wp_vocab, f\"vocab_wp_vocab_{args.vocab_size}.txt\")\n",
    "    print(\"Saved WordPiece-style merges and vocab for vocab-size target.\")\n",
    "\n",
    "    print(\"Done. Files written to current directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.argv = [\"train_subwords.py\", \"--input\", \"tokenized_bengali.txt\"]\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
