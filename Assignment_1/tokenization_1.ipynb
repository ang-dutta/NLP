{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68ea2d5",
   "metadata": {},
   "source": [
    "## Load Bengali dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dade7a4",
   "metadata": {},
   "source": [
    "- imports the ```load_dataset``` function from Hugging Face ```datasets``` library <br>\n",
    "- ```split=\"beb_Beng\"``` loads the bengali language subset of the dataset <br>\n",
    "- ```streaming=True``` this tells Hugging Face to stream the data lazily that means data is not fully downloaded into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a623fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "নয়াদিল্লি: আজ ভ্যালেন্টাইনস ডে। এমন একটি দিনে প্রিয় মানুষকে হৃদয় উজাড় করা ভালোবাসার বার্তা দিতে পৌঁছে দিতে উন্মুখ অনেকেই। এমন একটি দিনে ভারতীয় দলের প্রাক্তন ক্রিকেটার সচিন তেন্ডুলকর জানালেন তাঁর প্রথম ভালোবাসার কথা। একটি ভিডিও পোস্ট করেছেন ভারতীয় দলের প্রাক্তন ব্যাটিং স্তম্ভ। ভিডিওতে আন্তর্জাতিক ক্রিকেট ১০০ শতরানের মালিককে নেটে অনুশীলন করতে দেখা যাচ্ছে। ওই ভিডিও শেয়ার করে সচিন লিখেছেন, আমার প্রথম ভালোবাসা।\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"ben_Beng\", streaming=True)\n",
    "first_item = next(iter(dataset))\n",
    "print(first_item['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8b7d0",
   "metadata": {},
   "source": [
    "## Sentence Tokenizer and word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = re.split(r'[।.!?]+\\s*', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def tokenize_words(text):\n",
    "    pattern = r'''\n",
    "        \\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}     |  # Dates like 12/08/2025\n",
    "        \\d{4}-\\d{2}-\\d{2}                 |  # Dates like 2025-08-05\n",
    "        https?://\\S+                     |  # URLs with http or https\n",
    "        www\\.\\S+                         |  # URLs starting with www\n",
    "        [\\w._%+-]+@[\\w.-]+\\.\\w+          |  # Email addresses\n",
    "        \\d+\\.\\d+                         |  # Decimal numbers\n",
    "        \\d+                              |  # Whole numbers\n",
    "        [\\u0980-\\u09FF]+                 |  # Bengali words\n",
    "        [^\\s\\u0980-\\u09FF]               # Punctuation and symbols\n",
    "    '''\n",
    "    return re.findall(pattern, text, re.VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6d986",
   "metadata": {},
   "source": [
    "- ```import re``` python's regular expression module, used here for pattern matching <br>\n",
    "- Sentence tokenizer splits the paragraph into sentences using punctuation \n",
    "- Word tokenizer breaks the sentences into individual words called **tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff09f11",
   "metadata": {},
   "source": [
    "## Tokenization and writing into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"tokenized_bengali.txt\"\n",
    "\n",
    "sentence_list = []\n",
    "word_list = []\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    count = 0\n",
    "    for item in dataset:\n",
    "        paragraph = item[\"text\"]\n",
    "        sentences = tokenize_sentences(paragraph)\n",
    "\n",
    "        for sent in sentences:\n",
    "            words = tokenize_words(sent)\n",
    "            if words:\n",
    "                f_out.write(\" \".join(words) + \"\\n\")\n",
    "                sentence_list.append(words)\n",
    "                word_list.extend(words)\n",
    "\n",
    "        # Limit for quick testing\n",
    "        count += 1\n",
    "        if count >= 1000:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9256fa",
   "metadata": {},
   "source": [
    "- ```output_file = \"tokenized_bengali.txt\"``` all tokenized sentences will be saved here\n",
    "- sentence_list and word_list will store the tokens for later corpus statistics calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2650732",
   "metadata": {},
   "source": [
    "## Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b763ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(sentences, words):\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = len(words)\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "    avg_sentence_len = total_words / total_sentences if total_sentences > 0 else 0\n",
    "    avg_word_len = total_chars / total_words if total_words > 0 else 0\n",
    "    ttr = len(set(words)) / total_words if total_words > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'Total Sentences': total_sentences,\n",
    "        'Total Words': total_words,\n",
    "        'Total Characters': total_chars,\n",
    "        'Average Sentence Length': round(avg_sentence_len, 2),\n",
    "        'Average Word Length': round(avg_word_len, 2),\n",
    "        'Type/Token Ratio (TTR)': round(ttr, 4)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06876754",
   "metadata": {},
   "source": [
    "## Print Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b0f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Corpus Statistics ---\n",
      "Total Sentences: 1914\n",
      "Total Words: 24216\n",
      "Total Characters: 117045\n",
      "Average Sentence Length: 12.65\n",
      "Average Word Length: 4.83\n",
      "Type/Token Ratio (TTR): 0.3351\n"
     ]
    }
   ],
   "source": [
    "stats = compute_statistics(sentence_list, word_list)\n",
    "print(\"\\n--- Corpus Statistics ---\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
