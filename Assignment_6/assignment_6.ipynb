{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1f99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import heapq\n",
    "\n",
    "# Get n-grams\n",
    "def get_ngrams(sentences, n):\n",
    "    ngrams = []\n",
    "    for sent in sentences:\n",
    "        tokens = [\"<s>\"]*(n-1) + sent.strip().split() + [\"</s>\"]\n",
    "        ngrams.extend([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6885d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Katz Backoff (Quadrigram)\n",
    "class KatzBackoff:\n",
    "    def __init__(self, train, delta=0.5):\n",
    "        self.delta = delta\n",
    "        # build counts\n",
    "        self.unigrams = Counter(get_ngrams(train,1))\n",
    "        self.bigrams = Counter(get_ngrams(train,2))\n",
    "        self.trigrams = Counter(get_ngrams(train,3))\n",
    "        self.quads = Counter(get_ngrams(train,4))\n",
    "        self.total_unigrams = sum(self.unigrams.values())\n",
    "\n",
    "    def prob(self, w1,w2,w3,w4):\n",
    "        # Quadrigram MLE\n",
    "        quad = (w1,w2,w3,w4)\n",
    "        tri = (w1,w2,w3)\n",
    "        if self.quads[quad] > 0 and self.trigrams[tri] > 0:\n",
    "            return (self.quads[quad] - self.delta) / self.trigrams[tri]\n",
    "\n",
    "        # Backoff to trigram\n",
    "        tri2 = (w2,w3,w4)\n",
    "        bi = (w2,w3)\n",
    "        if self.trigrams[tri2] > 0 and self.bigrams[bi] > 0:\n",
    "            return (self.trigrams[tri2] - self.delta) / self.bigrams[bi]\n",
    "\n",
    "        # Backoff to bigram\n",
    "        bi2 = (w3,w4)\n",
    "        if self.bigrams[bi2] > 0 and self.unigrams[(w3,)] > 0:\n",
    "            return (self.bigrams[bi2] - self.delta) / self.unigrams[(w3,)]\n",
    "\n",
    "        # Backoff to unigram\n",
    "        return self.unigrams[(w4,)] / self.total_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d66327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Kneser-Ney Smoothing (Quadrigram)\n",
    "class KneserNey:\n",
    "    def __init__(self, train, discount=0.75):\n",
    "        self.d = discount\n",
    "        self.unigrams = Counter(get_ngrams(train,1))\n",
    "        self.bigrams = Counter(get_ngrams(train,2))\n",
    "        self.trigrams = Counter(get_ngrams(train,3))\n",
    "        self.quads = Counter(get_ngrams(train,4))\n",
    "        self.total_unigrams = sum(self.unigrams.values())\n",
    "\n",
    "        # continuation counts for lower-order models\n",
    "        self.continuation_counts = defaultdict(set)\n",
    "        for (w1,w2,w3,w4) in self.quads:\n",
    "            self.continuation_counts[(w2,w3,w4)].add(w4)\n",
    "\n",
    "    def prob_quad(self, w1,w2,w3,w4):\n",
    "        quad = (w1,w2,w3,w4)\n",
    "        tri = (w1,w2,w3)\n",
    "\n",
    "        # numerator with discount\n",
    "        numerator = max(self.quads[quad] - self.d, 0) / self.trigrams[tri] if self.trigrams[tri] > 0 else 0\n",
    "\n",
    "        # backoff weight\n",
    "        lambda_factor = (self.d * len([q for q in self.quads if q[:3]==tri])) / self.trigrams[tri] if self.trigrams[tri] > 0 else 1\n",
    "\n",
    "        # continuation probability (using trigram model)\n",
    "        cont_prob = self.prob_tri(w2,w3,w4)\n",
    "        return numerator + lambda_factor * cont_prob\n",
    "\n",
    "    def prob_tri(self, w2,w3,w4):\n",
    "        tri = (w2,w3,w4)\n",
    "        bi = (w2,w3)\n",
    "        numerator = max(self.trigrams[tri] - self.d, 0) / self.bigrams[bi] if self.bigrams[bi] > 0 else 0\n",
    "        lambda_factor = (self.d * len([t for t in self.trigrams if t[:2]==bi])) / self.bigrams[bi] if self.bigrams[bi] > 0 else 1\n",
    "        return numerator + lambda_factor * self.prob_bi(w3,w4)\n",
    "\n",
    "    def prob_bi(self, w3,w4):\n",
    "        bi = (w3,w4)\n",
    "        numerator = max(self.bigrams[bi] - self.d, 0) / self.unigrams[(w3,)] if self.unigrams[(w3,)] > 0 else 0\n",
    "        lambda_factor = (self.d * len([b for b in self.bigrams if b[0]==w3])) / self.unigrams[(w3,)] if self.unigrams[(w3,)] > 0 else 1\n",
    "        return numerator + lambda_factor * self.prob_uni(w4)\n",
    "\n",
    "    def prob_uni(self, w):\n",
    "        return self.unigrams[(w,)] / self.total_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b11194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sentence Generation\n",
    "\n",
    "# (a) Greedy Generation\n",
    "def generate_greedy(model, max_len=15):\n",
    "    sentence = [\"<s>\", \"<s>\", \"<s>\"]\n",
    "    for _ in range(max_len):\n",
    "        candidates = {}\n",
    "        for word in model.unigrams.keys():\n",
    "            prob = model.prob(sentence[-3], sentence[-2], sentence[-1], word[0]) if isinstance(model, KatzBackoff) else model.prob_quad(sentence[-3], sentence[-2], sentence[-1], word[0])\n",
    "            candidates[word[0]] = prob\n",
    "        next_word = max(candidates, key=candidates.get)  # pick word with highest prob\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        sentence.append(next_word)\n",
    "    return \" \".join(sentence[3:])  # remove <s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2c5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Beam Search\n",
    "def generate_beam(model, beam_size=20, max_len=15):\n",
    "    beam = [([\"<s>\", \"<s>\", \"<s>\"], 0.0)]  # (sequence, log-probability)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            for word in model.unigrams.keys():\n",
    "                prob = model.prob(seq[-3], seq[-2], seq[-1], word[0]) if isinstance(model, KatzBackoff) else model.prob_quad(seq[-3], seq[-2], seq[-1], word[0])\n",
    "                new_seq = seq + [word[0]]\n",
    "                new_score = score + math.log(prob + 1e-12)\n",
    "                new_beam.append((new_seq, new_score))\n",
    "        # keep top beam_size\n",
    "        beam = heapq.nlargest(beam_size, new_beam, key=lambda x: x[1])\n",
    "        # stop if all sequences ended\n",
    "        if all(seq[-1] == \"</s>\" for seq,_ in beam):\n",
    "            break\n",
    "\n",
    "    best_seq = max(beam, key=lambda x: x[1])[0]\n",
    "    return \" \".join(best_seq[3:-1])  # remove <s> and </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy (Katz): \n",
      "Beam Search (Katz): e d d a n e s h S c i e n c\n",
      "Greedy (Kneser-Ney): \n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    # load your corpus\n",
    "    with open(\"tokenized_bengali.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    # train models\n",
    "    katz = KatzBackoff(sentences)\n",
    "    kn = KneserNey(sentences)\n",
    "\n",
    "    # Generate sentences\n",
    "    print(\"Greedy (Katz):\", generate_greedy(katz))\n",
    "    print(\"Beam Search (Katz):\", generate_beam(katz))\n",
    "\n",
    "    print(\"Greedy (Kneser-Ney):\", generate_greedy(kn))\n",
    "    print(\"Beam Search (Kneser-Ney):\", generate_beam(kn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
