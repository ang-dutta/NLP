{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1914 sentences\n",
      "Built all n-gram models\n",
      "   Vocabulary size: 8114\n",
      "   Unigrams: 8114\n",
      "   Bigrams: 21574\n",
      "   Trigrams: 23163\n",
      "   Quadrigrams: 21914\n",
      "All models saved to files\n",
      "Smoothing results saved to 'smoothing_output.txt'\n"
     ]
    }
   ],
   "source": [
    "# for counting n-grams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Read tokenied text file and split each line into tokens\n",
    "def load_corpus(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    corpus = [line.strip().split() for line in lines if line.strip()]\n",
    "    return corpus\n",
    "\n",
    "# Build frequency counts for unigrams, bigrams, trigrams, and quadrigrams\n",
    "def build_ngram_models(corpus):\n",
    "    # Build unigram model - count each token\n",
    "    unigram_counts = Counter()\n",
    "    for sentence in corpus:\n",
    "        for token in sentence:\n",
    "            unigram_counts[token] += 1\n",
    "    \n",
    "    # Build bigram model - count pairs of tokens with sentence boundaries (<s>, </s>)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        tokens = ['<s>'] + sentence + ['</s>']\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "    \n",
    "    # Build trigram model\n",
    "    trigram_counts = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        tokens = ['<s>'] + sentence + ['</s>']\n",
    "        for i in range(len(tokens) - 2):\n",
    "            trigram = tuple(tokens[i:i+3])\n",
    "            trigram_counts[trigram] += 1\n",
    "    \n",
    "    # Build quadrigram model\n",
    "    quadrigram_counts = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        tokens = ['<s>'] + sentence + ['</s>']\n",
    "        for i in range(len(tokens) - 3):\n",
    "            quadrigram = tuple(tokens[i:i+4])\n",
    "            quadrigram_counts[quadrigram] += 1\n",
    "    \n",
    "    return unigram_counts, bigram_counts, trigram_counts, quadrigram_counts\n",
    "\n",
    "# Smoothing functions\n",
    "def add_one_smoothing(count, context_count, vocab_size):\n",
    "    return (count + 1) / (context_count + vocab_size)\n",
    "\n",
    "def add_k_smoothing(count, context_count, vocab_size, k):\n",
    "    return (count + k) / (context_count + k * vocab_size)\n",
    "\n",
    "def token_type_smoothing(count, context_count, vocab_size):\n",
    "    return (count + vocab_size) / (context_count + vocab_size * vocab_size)\n",
    "\n",
    "# Calculate unigram probabilities\n",
    "def get_unigram_prob(word, unigram_counts, vocab_size, smoothing='none', k=1):\n",
    "    count = unigram_counts[word]\n",
    "    total = sum(unigram_counts.values())\n",
    "    \n",
    "    if smoothing == 'add_one':\n",
    "        return add_one_smoothing(count, total, vocab_size)\n",
    "    elif smoothing == 'add_k':\n",
    "        return add_k_smoothing(count, total, vocab_size, k)\n",
    "    elif smoothing == 'token_type':\n",
    "        return token_type_smoothing(count, total, vocab_size)\n",
    "    else:\n",
    "        return count / total if total > 0 else 0\n",
    "\n",
    "# Calculate bigram probabilities\n",
    "def get_bigram_prob(bigram, bigram_counts, unigram_counts, vocab_size, smoothing='none', k=1):\n",
    "    count = bigram_counts[bigram]\n",
    "    context_count = unigram_counts[bigram[0]]\n",
    "    \n",
    "    if smoothing == 'add_one':\n",
    "        return add_one_smoothing(count, context_count, vocab_size)\n",
    "    elif smoothing == 'add_k':\n",
    "        return add_k_smoothing(count, context_count, vocab_size, k)\n",
    "    elif smoothing == 'token_type':\n",
    "        return token_type_smoothing(count, context_count, vocab_size)\n",
    "    else:\n",
    "        return count / context_count if context_count > 0 else 0\n",
    "\n",
    "# Calculate trigram probabilities\n",
    "def get_trigram_prob(trigram, trigram_counts, bigram_counts, vocab_size, smoothing='none', k=1):\n",
    "    count = trigram_counts[trigram]\n",
    "    context = (trigram[0], trigram[1])\n",
    "    context_count = bigram_counts[context]\n",
    "    \n",
    "    if smoothing == 'add_one':\n",
    "        return add_one_smoothing(count, context_count, vocab_size)\n",
    "    elif smoothing == 'add_k':\n",
    "        return add_k_smoothing(count, context_count, vocab_size, k)\n",
    "    elif smoothing == 'token_type':\n",
    "        return token_type_smoothing(count, context_count, vocab_size)\n",
    "    else:\n",
    "        return count / context_count if context_count > 0 else 0\n",
    "\n",
    "# Calculate quadrigram probabilities\n",
    "def get_quadrigram_prob(quadrigram, quadrigram_counts, trigram_counts, vocab_size, smoothing='none', k=1):\n",
    "    count = quadrigram_counts[quadrigram]\n",
    "    context = (quadrigram[0], quadrigram[1], quadrigram[2])\n",
    "    context_count = trigram_counts[context]\n",
    "    \n",
    "    if smoothing == 'add_one':\n",
    "        return add_one_smoothing(count, context_count, vocab_size)\n",
    "    elif smoothing == 'add_k':\n",
    "        return add_k_smoothing(count, context_count, vocab_size, k)\n",
    "    elif smoothing == 'token_type':\n",
    "        return token_type_smoothing(count, context_count, vocab_size)\n",
    "    else:\n",
    "        return count / context_count if context_count > 0 else 0\n",
    "\n",
    "def save_model_to_file(model_counts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for ngram, count in model_counts.items():\n",
    "            if isinstance(ngram, tuple):\n",
    "                ngram_str = ' '.join(ngram)\n",
    "            else:\n",
    "                ngram_str = str(ngram)\n",
    "            f.write(f\"{ngram_str}\\t{count}\\n\")\n",
    "\n",
    "# Save smoothing results\n",
    "def save_smoothing_results(bigram_counts, unigram_counts, vocab_size):\n",
    "    k = 0.3\n",
    "    with open('smoothing_output.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Bigram Language Model Smoothing Results\\n\")\n",
    "        f.write(\"Formulas:\\n\")\n",
    "        f.write(\"1. Add-One: P(w2|w1) = (count(w1,w2) + 1) / (count(w1) + V)\\n\")\n",
    "        f.write(f\"2. Add-K: P(w2|w1) = (count(w1,w2) + {k}) / (count(w1) + {k}*V)\\n\")\n",
    "        f.write(\"3. Token Type: P(w2|w1) = (count(w1,w2) + V) / (count(w1) + VÂ²)\\n\\n\")\n",
    "        f.write(\"Bigram\\tCount\\tContextCount\\tAddOne\\tAddK\\tTokenType\\n\")\n",
    "        \n",
    "        for bigram in bigram_counts:\n",
    "            count = bigram_counts[bigram]\n",
    "            context_count = unigram_counts[bigram[0]]\n",
    "            \n",
    "            p_add_one = add_one_smoothing(count, context_count, vocab_size)\n",
    "            p_add_k = add_k_smoothing(count, context_count, vocab_size, k)\n",
    "            p_token_type = token_type_smoothing(count, context_count, vocab_size)\n",
    "            \n",
    "            f.write(f\"{bigram}\\t{count}\\t{context_count}\\t{p_add_one:.6f}\\t{p_add_k:.6f}\\t{p_token_type:.6f}\\n\")\n",
    "\n",
    "def main():\n",
    "    filepath = 'tokenized_bengali.txt'\n",
    "    \n",
    "    # Load corpus\n",
    "    corpus = load_corpus(filepath)\n",
    "    print(f\"Loaded {len(corpus)} sentences\")\n",
    "    \n",
    "    # Build all models\n",
    "    unigram_counts, bigram_counts, trigram_counts, quadrigram_counts = build_ngram_models(corpus)\n",
    "    \n",
    "    # Get vocabulary\n",
    "    vocab = set(token for sentence in corpus for token in sentence)\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    print(f\"Built all n-gram models\")\n",
    "    print(f\"   Vocabulary size: {vocab_size}\")\n",
    "    print(f\"   Unigrams: {len(unigram_counts)}\")\n",
    "    print(f\"   Bigrams: {len(bigram_counts)}\")\n",
    "    print(f\"   Trigrams: {len(trigram_counts)}\")\n",
    "    print(f\"   Quadrigrams: {len(quadrigram_counts)}\")\n",
    "    \n",
    "    save_model_to_file(unigram_counts, \"unigram_model.txt\")\n",
    "    save_model_to_file(bigram_counts, \"bigram_model.txt\")\n",
    "    save_model_to_file(trigram_counts, \"trigram_model.txt\")\n",
    "    save_model_to_file(quadrigram_counts, \"quadrigram_model.txt\")\n",
    "    print(\"All models saved to files\")\n",
    "    \n",
    "    # Save smoothing results\n",
    "    save_smoothing_results(bigram_counts, unigram_counts, vocab_size)\n",
    "    print(\"Smoothing results saved to 'smoothing_output.txt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
