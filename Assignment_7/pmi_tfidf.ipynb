{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b07093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for shuffling data before splitting\n",
    "import random\n",
    "#for shuffling data before splitting\n",
    "import math\n",
    "#efficiently count tokens and bigrams\n",
    "from collections import Counter\n",
    "#numerical operations\n",
    "import numpy as np\n",
    "#converts sentences into TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#computes pairwise sentence similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#efficiently find nearest neighbors\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c5ebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences loaded: 19772\n"
     ]
    }
   ],
   "source": [
    "#Open tokenized Bengali where Each line = one sentence\n",
    "with open(\"tokenized_bengali.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Total sentences loaded: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f9e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15817, Validation: 1977, Test: 1978\n"
     ]
    }
   ],
   "source": [
    "#Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "#Shuffle sentences to ensure random distribution\n",
    "random.shuffle(sentences)\n",
    "\n",
    "#Split data into 80% train, 10% validation, 10% test\n",
    "train_size = int(0.8 * len(sentences))\n",
    "valid_size = int(0.1 * len(sentences))\n",
    "\n",
    "train_sents = sentences[:train_size]\n",
    "valid_sents = sentences[train_size:train_size + valid_size]\n",
    "test_sents  = sentences[train_size + valid_size:]\n",
    "\n",
    "print(f\"Train: {len(train_sents)}, Validation: {len(valid_sents)}, Test: {len(test_sents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab150dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique unigrams: 34246\n",
      "Total unique bigrams : 144760\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(sentences):\n",
    "    #Flattens all tokens from the list of sentences into a single list\n",
    "    return [tok for sent in sentences for tok in sent.split()]\n",
    "\n",
    "# Unigrams and bigrams from training set\n",
    "unigrams = get_tokens(train_sents)\n",
    "bigrams = [(w1, w2) for sent in train_sents for w1, w2 in zip(sent.split()[:-1], sent.split()[1:])]\n",
    "\n",
    "#Counts token and bigram frequencies\n",
    "uni_counts = Counter(unigrams)\n",
    "bi_counts  = Counter(bigrams)\n",
    "\n",
    "total_unigrams = sum(uni_counts.values())\n",
    "total_bigrams  = sum(bi_counts.values())\n",
    "\n",
    "print(f\"Total unique unigrams: {len(uni_counts)}\")\n",
    "print(f\"Total unique bigrams : {len(bi_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2100830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed PMI for 6017 validation bigrams and 5981 test bigrams.\n",
      "PMI scores saved as pmi_validation.txt and pmi_test.txt\n"
     ]
    }
   ],
   "source": [
    "def compute_pmi(bigrams_list, uni_counts, bi_counts, total_unigrams, total_bigrams):\n",
    "    \"\"\"\n",
    "    Compute PMI\n",
    "    For each bigram (w1, w2):\n",
    "    PMI(w1, w2) = log2( P(w1, w2) / (P(w1) * P(w2)) )\n",
    "    where:\n",
    "     - P(w1) = count(w1) / total_unigrams\n",
    "     - P(w2) = count(w2) / total_unigrams\n",
    "     - P(w1, w2) = count(w1, w2) / total_bigrams\n",
    "     - Returns a dict mapping bigrams to their PMI scores.\n",
    "    \"\"\"\n",
    "    pmi_scores = {}\n",
    "    for w1, w2 in bigrams_list:\n",
    "        if (w1, w2) in bi_counts and uni_counts[w1] > 0 and uni_counts[w2] > 0:\n",
    "            p_w1 = uni_counts[w1] / total_unigrams\n",
    "            p_w2 = uni_counts[w2] / total_unigrams\n",
    "            p_w1w2 = bi_counts[(w1, w2)] / total_bigrams\n",
    "            pmi = math.log2(p_w1w2 / (p_w1 * p_w2))\n",
    "            pmi_scores[(w1, w2)] = pmi\n",
    "    return pmi_scores\n",
    "\n",
    "# Collect bigrams from val/test and compute PMI using training counts\n",
    "valid_bigrams = [(w1, w2) for sent in valid_sents for w1, w2 in zip(sent.split()[:-1], sent.split()[1:])]\n",
    "test_bigrams  = [(w1, w2) for sent in test_sents  for w1, w2 in zip(sent.split()[:-1], sent.split()[1:])]\n",
    "\n",
    "pmi_valid = compute_pmi(valid_bigrams, uni_counts, bi_counts, total_unigrams, total_bigrams)\n",
    "pmi_test  = compute_pmi(test_bigrams, uni_counts, bi_counts, total_unigrams, total_bigrams)\n",
    "\n",
    "print(f\"Computed PMI for {len(pmi_valid)} validation bigrams and {len(pmi_test)} test bigrams.\")\n",
    "\n",
    "# Save PMI results\n",
    "with open(\"pmi_validation.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for (w1, w2), score in sorted(pmi_valid.items(), key=lambda x: -x[1]):\n",
    "        f.write(f\"{w1} {w2}\\t{score:.4f}\\n\")\n",
    "\n",
    "with open(\"pmi_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for (w1, w2), score in sorted(pmi_test.items(), key=lambda x: -x[1]):\n",
    "        f.write(f\"{w1} {w2}\\t{score:.4f}\\n\")\n",
    "\n",
    "print(\"PMI scores saved as pmi_validation.txt and pmi_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6b3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shapes -> Train: (15817, 5217) , Valid: (1977, 5217) , Test: (1978, 5217)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")  # keeps Bengali words\n",
    "tfidf_train = vectorizer.fit_transform(train_sents)\n",
    "tfidf_valid = vectorizer.transform(valid_sents)\n",
    "tfidf_test  = vectorizer.transform(test_sents)\n",
    "\n",
    "print(\"TF-IDF shapes -> Train:\", tfidf_train.shape, \", Valid:\", tfidf_valid.shape, \", Test:\", tfidf_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a3128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors_within(tfidf_matrix):\n",
    "    \"\"\"Find nearest neighbor within the same set.\"\"\"\n",
    "    sim = cosine_similarity(tfidf_matrix)\n",
    "    np.fill_diagonal(sim, -1)  # exclude self\n",
    "    nn_indices = np.argmax(sim, axis=1)\n",
    "    return nn_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d5ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nn = nearest_neighbors_within(tfidf_valid)\n",
    "test_nn  = nearest_neighbors_within(tfidf_test)\n",
    "\n",
    "# Save nearest neighbor results\n",
    "with open(\"nearest_neighbors_validation.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, idx in enumerate(valid_nn):\n",
    "        f.write(f\"Sentence {i} -> NN Sentence {idx}\\n\")\n",
    "\n",
    "with open(\"nearest_neighbors_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, idx in enumerate(test_nn):\n",
    "        f.write(f\"Sentence {i} -> NN Sentence {idx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8713a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's NearestNeighbors (brute-force cosine)\n",
    "nn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "nn_model.fit(tfidf_train)\n",
    "\n",
    "# For validation sentences\n",
    "dist_val, idx_val = nn_model.kneighbors(tfidf_valid, n_neighbors=1)\n",
    "# For test sentences\n",
    "dist_test, idx_test = nn_model.kneighbors(tfidf_test, n_neighbors=1)\n",
    "\n",
    "# Save results\n",
    "with open(\"nn_val_to_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, (dist, idx) in enumerate(zip(dist_val, idx_val)):\n",
    "        f.write(f\"Validation sentence {i} -> Train sentence {idx[0]} (cosine distance={dist[0]:.4f})\\n\")\n",
    "\n",
    "with open(\"nn_test_to_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, (dist, idx) in enumerate(zip(dist_test, idx_test)):\n",
    "        f.write(f\"Test sentence {i} -> Train sentence {idx[0]} (cosine distance={dist[0]:.4f})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
